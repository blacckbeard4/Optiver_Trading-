{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30553,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Optiver Trading at the Close: GRU-based Time Series Forecasting\n\nThis notebook implements a full pipeline to forecast auction price movements using a GRU-based neural network model. The pipeline includes data cleaning, feature engineering, sequence preparation, model definition, hyperparameter tuning with Optuna, and final live predictions using the optiver2023 test API.","metadata":{}},{"cell_type":"markdown","source":"## 1. Environment Setup & Data Loading\n\nWe load the Optiver dataset using Polars for performance and initialize the test environment for live predictions.","metadata":{}},{"cell_type":"code","source":"import optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2025-05-04T15:19:30.585206Z","iopub.execute_input":"2025-05-04T15:19:30.585555Z","iopub.status.idle":"2025-05-04T15:19:30.900617Z","shell.execute_reply.started":"2025-05-04T15:19:30.585509Z","shell.execute_reply":"2025-05-04T15:19:30.899780Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Exploration\n\nWe summarize missing values in each column and visualize the distribution of the target variable to understand its range and behavior","metadata":{}},{"cell_type":"code","source":"import polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd \n\n# Load the dataset\ndf = pl.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n\n# ---- Compute null summary ----\nnulls = (\n    pl.DataFrame({\n        \"column\": df.columns,\n        \"null_count\": df.null_count().row(0)\n    })\n    .with_columns([\n        (pl.col(\"null_count\") / df.height * 100).alias(\"null_pct\")\n    ])\n    .sort(\"null_count\", descending=True)\n)\n\nprint(\"üìä Null Summary:\")\nprint(nulls)\n\n# ---- Plot target distribution ----\nplt.figure(figsize=(10, 4))\nsns.histplot(df.select(\"target\").to_pandas()[\"target\"], bins=100, kde=True)\nplt.title(\"Target Distribution\")\nplt.xlabel(\"Target Value\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:19:30.902394Z","iopub.execute_input":"2025-05-04T15:19:30.902839Z","iopub.status.idle":"2025-05-04T15:19:57.361153Z","shell.execute_reply.started":"2025-05-04T15:19:30.902808Z","shell.execute_reply":"2025-05-04T15:19:57.360232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Cleaning\n\nWe:\n\t‚Ä¢\tRemove unusable columns (far_price, near_price)\n\t‚Ä¢\tImpute missing values with column-wise means\n\t‚Ä¢\tReplace infinite values with 0\n\nTwo versions of the cleaning function are defined for train and test sets.","metadata":{}},{"cell_type":"code","source":"def clean_data(df: pl.DataFrame) -> pl.DataFrame:\n    df = df.filter(pl.col(\"target\").is_not_null() & ~pl.col(\"target\").is_nan())\n\n    # Drop unused columns\n    df = df.drop([\"far_price\", \"near_price\"])\n\n    # Compute global means for fallback\n    global_means = {\n        col: df.select(pl.col(col).mean()).to_series()[0]\n        for col, dtype in df.schema.items()\n        if dtype in [pl.Float64, pl.Int64] and col != \"target\"\n    }\n\n    for col, dtype in df.schema.items():\n        if dtype in [pl.Float64, pl.Int64] and col != \"target\":\n            # Fill with group mean, fallback to global mean\n            df = df.with_columns(\n                pl.when(pl.col(col).is_null())\n                .then(\n                    pl.col(col)\n                    .mean()\n                    .over([\"stock_id\", \"date_id\"])\n                    .fill_null(global_means[col])\n                )\n                .otherwise(pl.col(col))\n                .alias(col)\n            )\n\n        if dtype == pl.Float64:\n            # Replace infinite values with 0.0\n            df = df.with_columns(\n                pl.when(pl.col(col).is_infinite()).then(0.0).otherwise(pl.col(col)).alias(col)\n            )\n\n    return df\n\n\ndf_clean = clean_data(df)\n\ndef clean_data_test(df: pl.DataFrame) -> pl.DataFrame:\n    df = df.drop([\"far_price\", \"near_price\"])\n\n    # Compute global means\n    global_means = {\n        col: df.select(pl.col(col).mean()).to_series()[0]\n        for col, dtype in df.schema.items()\n        if dtype in [pl.Float64, pl.Int64]\n    }\n\n    for col, dtype in df.schema.items():\n        if dtype in [pl.Float64, pl.Int64]:\n            # Fill with group mean and fallback to global mean\n            df = df.with_columns(\n                pl.when(pl.col(col).is_null())\n                .then(\n                    pl.col(col)\n                    .mean()\n                    .over([\"stock_id\", \"date_id\"])\n                    .fill_null(global_means[col])\n                )\n                .otherwise(pl.col(col))\n                .alias(col)\n            )\n\n        if dtype == pl.Float64:\n            df = df.with_columns(\n                pl.when(pl.col(col).is_infinite()).then(0.0).otherwise(pl.col(col)).alias(col)\n            )\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:52:54.623910Z","iopub.execute_input":"2025-05-04T19:52:54.624156Z","iopub.status.idle":"2025-05-04T19:52:54.874210Z","shell.execute_reply.started":"2025-05-04T19:52:54.624123Z","shell.execute_reply":"2025-05-04T19:52:54.872993Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Sequence Preparation for GRU Input\n\nIn this step, we reshape the tabular time-series data into 3D padded tensors that are compatible with GRU input requirements:\n\t‚Ä¢\tEach sequence is grouped by (stock_id, date_id) and sorted by seconds_in_bucket.\n\t‚Ä¢\tFeatures (X) and targets (y) are extracted and padded to the same maximum sequence length across batches using a placeholder value (-9999).\n\t‚Ä¢\tA companion tensor of sequence lengths is returned to handle variable-length sequences during training using pack_padded_sequence.\n\nWe define two utility functions:\n\t‚Ä¢\tprepare_gru_inputs for training and validation (returns X, y, and lengths).\n\t‚Ä¢\tprepare_gru_inputs_test for test-time inference (also returns row IDs to match predictions back to the original rows).\n\nThis step ensures that the data is correctly structured for temporal modeling using GRUs.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\n\nfeature_cols = [\n    \"imbalance_size\", \"matched_size\", \"bid_size\", \"ask_size\",\n    \"reference_price\", \"bid_price\", \"ask_price\", \"wap\", \"imbalance_buy_sell_flag\"\n]\n\ndef prepare_gru_inputs(df, feature_cols, target_col=\"target\", pad_value=-9999):\n    X_batches, y_batches, lengths = [], [], []\n    for (stock_id, date_id), group in df.groupby([\"stock_id\", \"date_id\"]):\n        group_sorted = group.sort_values(\"seconds_in_bucket\")\n        X = group_sorted[feature_cols].to_numpy(dtype=np.float32)\n        y = group_sorted[target_col].to_numpy(dtype=np.float32)\n\n        lengths.append(len(X))\n        X_batches.append(X)\n        y_batches.append(y)\n\n    max_len = max(lengths)\n    X_padded = np.full((len(X_batches), max_len, len(feature_cols)), pad_value, dtype=np.float32)\n    y_padded = np.full((len(y_batches), max_len), pad_value, dtype=np.float32)\n\n    for i, (x, y) in enumerate(zip(X_batches, y_batches)):\n        X_padded[i, :len(x), :] = x\n        y_padded[i, :len(y)] = y\n\n    return torch.tensor(X_padded), torch.tensor(y_padded), torch.tensor(lengths)\n\n\n\ndef prepare_gru_inputs_test(df, feature_cols, pad_value=-9999):\n    X_batches, lengths, ids = [], [], []\n    for (stock_id, date_id), group in df.groupby([\"stock_id\", \"date_id\"]):\n        group_sorted = group.sort_values(\"seconds_in_bucket\")\n        X = group_sorted[feature_cols].to_numpy(dtype=np.float32)\n\n        lengths.append(len(X))\n        X_batches.append(X)\n        ids.append(group_sorted[\"row_id\"].tolist())  # store row_ids to map back\n\n    max_len = max(lengths)\n    X_padded = np.full((len(X_batches), max_len, len(feature_cols)), pad_value, dtype=np.float32)\n\n    for i, x in enumerate(X_batches):\n        X_padded[i, :len(x), :] = x\n\n    return torch.tensor(X_padded), torch.tensor(lengths), ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:52:54.875167Z","iopub.status.idle":"2025-05-04T19:52:54.875521Z","shell.execute_reply.started":"2025-05-04T19:52:54.875337Z","shell.execute_reply":"2025-05-04T19:52:54.875352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Time Series-Aware Train/Validation Split\n\nWe perform a date-based split to ensure that the validation set only contains future unseen dates.","metadata":{}},{"cell_type":"code","source":"def time_series_split_df(df, split_ratio=0.85):\n    unique_dates = sorted(df[\"date_id\"].unique())\n    split_idx = int(len(unique_dates) * split_ratio)\n    train_dates = unique_dates[:split_idx]\n    valid_dates = unique_dates[split_idx:]\n    \n    train_df = df[df[\"date_id\"].isin(train_dates)].copy()\n    valid_df = df[df[\"date_id\"].isin(valid_dates)].copy()\n    return train_df, valid_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:02.096493Z","iopub.execute_input":"2025-05-04T15:20:02.096881Z","iopub.status.idle":"2025-05-04T15:20:02.101693Z","shell.execute_reply.started":"2025-05-04T15:20:02.096858Z","shell.execute_reply":"2025-05-04T15:20:02.100687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. GRU Model Definition\n\nWe implement a configurable GRU model that supports:\n\t‚Ä¢\tVariable GRU layers\n\t‚Ä¢\tLayerNorm and Dropout\n\t‚Ä¢\tMultiple Dense layers with GELU activation\n\t‚Ä¢\tFinal regression output","metadata":{}},{"cell_type":"code","source":"import optuna\nimport time\nfrom tqdm import tqdm\nfrom optuna.exceptions import TrialPruned\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\nclass GRURegressor(nn.Module):\n    def __init__(self, input_dim, hidden_dim, dropout, num_dense_layers,gru_layers):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=gru_layers, batch_first=True, bidirectional=False)\n        self.norm = nn.LayerNorm(hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n\n        layers = []\n        dim = hidden_dim\n        for _ in range(num_dense_layers):\n            layers += [\n                nn.Linear(dim, dim // 2),\n                nn.GELU(),\n                nn.Dropout(dropout)\n            ]\n            dim = dim // 2\n        layers.append(nn.Linear(dim, 1))\n        self.head = nn.Sequential(*layers)\n\n    def forward(self, x, lengths):\n        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        packed_out, _ = self.gru(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n        out = self.norm(out)\n        out = self.dropout(out)\n        return self.head(out).squeeze(-1)\n\ndef objective(trial):\n    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 160, 192,256])\n    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n    lr = trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True)\n    beta = trial.suggest_float(\"beta\", 0.7, 1.0)\n    num_dense_layers = trial.suggest_int(\"num_dense_layers\", 0, 4)\n    num_epochs = trial.suggest_int(\"num_epochs\", 5, 50)\n    gru_layers = trial.suggest_int(\"gru_layers\", 1, 3)\n\n    model = GRURegressor(len(feature_cols), hidden_dim, dropout, num_dense_layers,gru_layers).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.SmoothL1Loss(beta=beta)\n\n    for epoch in range(num_epochs):\n        model.train()\n        for Xb, yb, lb in train_loader:\n            Xb, yb, lb = Xb.to(device), yb.to(device), lb.to(device)\n            pred = model(Xb, lb)\n            mask = yb != -9999\n            loss = loss_fn(pred[mask], yb[mask])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for Xb, yb, lb in val_loader:\n                Xb, yb, lb = Xb.to(device), yb.to(device), lb.to(device)\n                pred = model(Xb, lb)\n                mask = yb != -9999\n                loss = loss_fn(pred[mask], yb[mask])\n                val_loss += loss.item()\n        val_loss /= len(val_loader)\n\n        trial.report(val_loss, step=epoch)\n        if trial.should_prune():\n            raise TrialPruned()\n\n    return val_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:02.102818Z","iopub.execute_input":"2025-05-04T15:20:02.103043Z","iopub.status.idle":"2025-05-04T15:20:03.050671Z","shell.execute_reply.started":"2025-05-04T15:20:02.103023Z","shell.execute_reply":"2025-05-04T15:20:03.049785Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Hyperparameter Tuning with Optuna\n\nWe define an objective function that trains the model and evaluates MAE on the validation set. Optuna explores combinations of:\n\t‚Ä¢\tHidden size\n\t‚Ä¢\tDropout rate\n\t‚Ä¢\tLearning rate\n\t‚Ä¢\tNumber of dense/GRU layers\n\t‚Ä¢\tEpochs\n\t‚Ä¢\tSmooth L1 loss beta parameter","metadata":{}},{"cell_type":"code","source":"# Step 1: Convert to pandas\ndf_clean_pd = df_clean.to_pandas()\n\n# Step 2: Time Series Split (returns train_df and val_df)\ntrain_df, val_df = time_series_split_df(df_clean_pd)\n\n# Step 3: Normalize features based on train set statistics\nscaler = StandardScaler()\ntrain_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\nval_df[feature_cols] = scaler.transform(val_df[feature_cols])\n\n# Optional: Save the scaler if you need it at test time\n\njoblib.dump(scaler, \"scaler.pkl\")\n\n# Step 4: Prepare GRU inputs\n(train_X, train_y, train_len) = prepare_gru_inputs(train_df, feature_cols)\n(val_X, val_y, val_len) = prepare_gru_inputs(val_df, feature_cols)\n\n\n\n\ntrain_ds = TensorDataset(train_X, train_y, train_len)\nval_ds = TensorDataset(val_X, val_y, val_len)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Run optimization with progress bar\nstudy = optuna.create_study(direction=\"minimize\")\nprint(\"üß™ Starting hyperparameter tuning...\")\nstart = time.time()\nfor _ in tqdm(range(50), desc=\"Optuna Trials\"):\n    study.optimize(objective, n_trials=1, catch=(TrialPruned,))\nend = time.time()\n\nprint(\"‚úÖ Best trial:\")\nprint(study.best_trial.params)\nprint(f\"Best validation MAE: {study.best_trial.value:.5f}\")\nprint(f\"‚è± Total tuning time: {(end - start) / 60:.2f} min\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:03.051869Z","iopub.execute_input":"2025-05-04T15:20:03.052091Z","iopub.status.idle":"2025-05-04T16:30:14.657284Z","shell.execute_reply.started":"2025-05-04T15:20:03.052072Z","shell.execute_reply":"2025-05-04T16:30:14.656469Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Final Model Training\n\nUsing the best hyperparameters found, we retrain the GRU model on the full dataset and save the weights for prediction.","metadata":{}},{"cell_type":"code","source":"best_params = study.best_trial.params\n\n# Prepare full dataset\ndf_full = df_clean.to_pandas()\n\n# Normalize using StandardScaler\nscaler = StandardScaler()\ndf_full[feature_cols] = scaler.fit_transform(df_full[feature_cols])\njoblib.dump(scaler, \"scaler_full.pkl\")\n\n# Prepare GRU inputs\nfull_X, full_y, full_len = prepare_gru_inputs(df_full, feature_cols)\nfull_ds = TensorDataset(full_X, full_y, full_len)\nfull_loader = DataLoader(full_ds, batch_size=64, shuffle=False)\n\n# Initialize model with ALL best Optuna params (including gru_layers)\nmodel = GRURegressor(\n    input_dim=len(feature_cols),\n    hidden_dim=best_params[\"hidden_dim\"],\n    dropout=best_params[\"dropout\"],\n    num_dense_layers=best_params[\"num_dense_layers\"],\n    gru_layers=best_params[\"gru_layers\"]  \n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\nloss_fn = nn.SmoothL1Loss(beta=best_params[\"beta\"])\n\n# Final Training Loop\nfor epoch in range(best_params[\"num_epochs\"]):\n    model.train()\n    for Xb, yb, lb in full_loader:\n        Xb, yb, lb = Xb.to(device), yb.to(device), lb.to(device)\n        pred = model(Xb, lb)\n        mask = yb != -9999\n        loss = loss_fn(pred[mask], yb[mask])\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:30:14.658402Z","iopub.execute_input":"2025-05-04T16:30:14.658649Z","iopub.status.idle":"2025-05-04T16:38:09.634403Z","shell.execute_reply.started":"2025-05-04T16:30:14.658626Z","shell.execute_reply":"2025-05-04T16:38:09.633751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model\nmodel_path = \"best_gru_model.pth\"\ntorch.save(model.state_dict(), model_path)\nprint(f\"‚úÖ Model saved to {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:38:09.635331Z","iopub.execute_input":"2025-05-04T16:38:09.635564Z","iopub.status.idle":"2025-05-04T16:38:09.645554Z","shell.execute_reply.started":"2025-05-04T16:38:09.635545Z","shell.execute_reply":"2025-05-04T16:38:09.644763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Live Predictions with optiver2023 API\n\nFor each batch of test data:\n\t‚Ä¢\tWe clean and transform the input\n\t‚Ä¢\tPrepare padded sequences\n\t‚Ä¢\tPredict with the trained GRU model\n\t‚Ä¢\tFill predictions in sample_prediction and submit","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"best_gru_model.pth\", map_location=device))\nmodel.eval()\n\ncounter = 0\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    if counter == 0:\n        print(\"üì¶ Test preview:\")\n        print(test.head(3))\n        print(revealed_targets.head(3))\n        print(sample_prediction.head(3))\n\n    # 1. Clean the test set\n    test_pl = pl.from_pandas(test)\n    test_pl = clean_data_test(test_pl)\n    test_df = test_pl.to_pandas()\n    test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n\n    # 2. GRU input preparation (sequence format)\n    X_test, len_test, row_ids = prepare_gru_inputs_test(test_df, feature_cols)\n\n    # 3. Predict\n    with torch.no_grad():\n        X_test = X_test.to(device)\n        len_test = len_test.to(device)\n        preds = model(X_test, len_test).cpu().numpy().flatten()\n\n    # 4. Fill predictions into sample_prediction\n    flat_row_ids = [rid for sublist in row_ids for rid in sublist]\n    pred_df = pd.DataFrame({\"row_id\": flat_row_ids, \"target\": preds})\n    sample_prediction = sample_prediction.drop(columns=[\"target\"]).merge(pred_df, on=\"row_id\", how=\"left\")\n    sample_prediction[\"target\"] = sample_prediction[\"target\"].fillna(0.0)\n\n    # 5. Submit\n    env.predict(sample_prediction)\n    counter += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:38:09.648843Z","iopub.execute_input":"2025-05-04T16:38:09.649126Z","iopub.status.idle":"2025-05-04T16:38:38.511658Z","shell.execute_reply.started":"2025-05-04T16:38:09.649095Z","shell.execute_reply":"2025-05-04T16:38:38.510997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Save Submission File\n\nFinally, we export the sample_prediction.csv file and move it to the working directory for submission.","metadata":{}},{"cell_type":"code","source":"# Save sample_prediction to CSV\nsample_prediction.to_csv(\"sample_prediction.csv\", index=False)\n\n# Create a download link (Kaggle will show it in the sidebar under 'Output')\nimport shutil\nshutil.move(\"sample_prediction.csv\", \"/kaggle/working/sample_prediction.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:38:38.512676Z","iopub.execute_input":"2025-05-04T16:38:38.513018Z","iopub.status.idle":"2025-05-04T16:38:38.520608Z","shell.execute_reply.started":"2025-05-04T16:38:38.512988Z","shell.execute_reply":"2025-05-04T16:38:38.519782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}